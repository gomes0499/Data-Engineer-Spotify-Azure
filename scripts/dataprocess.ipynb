{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "004d7e3a-662c-437d-a209-e8b51bd4694b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-storage-blob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54e6394d-3f5a-4213-b6b2-7360bfc1528e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from azure.storage.blob import BlobServiceClient, ContentSettings\n",
    "import os\n",
    "from io import BytesIO\n",
    "import pandas\n",
    "\n",
    "# Replace these variables with your Blob Storage account details\n",
    "account_name = \"wu3storage\"\n",
    "access_key = \"your-access-key\"\n",
    "container_name = \"datalake\"\n",
    "input_folder = \"landing\"\n",
    "output_folder = \"process\"\n",
    "\n",
    "# Configure the Spark session to access Blob Storage\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AzureBlobStorage\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-azure:3.2.0\") \\\n",
    "    .config(\"spark.hadoop.fs.azure\", \"org.apache.hadoop.fs.azure.NativeAzureFileSystem\") \\\n",
    "    .config(\"spark.hadoop.fs.azure.account.key.\" + account_name + \".blob.core.windows.net\", access_key) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "\n",
    "# Create a list with the names of the tables you want to read\n",
    "tables = [\"processedArtists\", \"processedListeningHistory\", \"processedSongs\", \"processedUserPreferences\", \"processedUsers\"]\n",
    "\n",
    "\n",
    "# Loop through each table and create a DataFrame\n",
    "dfs = []\n",
    "for table in tables:\n",
    "    input_path = f\"wasbs://{container_name}@{account_name}.blob.core.windows.net/{input_folder}/{table}.parquet\"\n",
    "    df = spark.read.parquet(input_path)\n",
    "    dfs.append(df)\n",
    "    \n",
    "blob_service = BlobServiceClient(account_url=f\"https://{account_name}.blob.core.windows.net\", credential=access_key)\n",
    "\n",
    "# # Loop through all dataframes and apply transformations\n",
    "for i, df in enumerate(dfs):\n",
    "    print(f\"Processing DataFrame {i + 1}\")\n",
    "\n",
    "    # Remove rows with missing values\n",
    "    df = df.dropna()\n",
    "\n",
    "    # Convert Spark DataFrame to Pandas DataFrame\n",
    "    pdf = df.toPandas()\n",
    "\n",
    "    # Save the DataFrame to a temporary Parquet file in memory and upload it\n",
    "    with BytesIO() as temp_file:\n",
    "        pdf.to_parquet(temp_file, engine='pyarrow')\n",
    "        temp_file.seek(0)\n",
    "\n",
    "        # Upload the Parquet file to Azure Blob Storage\n",
    "        output_path = f\"{output_folder}/{tables[i]}.parquet\"\n",
    "        container_client = blob_service.get_container_client(container_name)\n",
    "        blob_client = container_client.get_blob_client(output_path)\n",
    "\n",
    "        blob_client.upload_blob(temp_file, overwrite=True)\n",
    "\n",
    "    print(f\"DataFrame {i + 1} processed successfully!\")\n",
    "\n",
    "print(\"All DataFrames processed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e641e79-8c27-4a99-bdd3-bd24ed386540",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "dataprocess",
   "notebookOrigID": 2754360996950559,
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
